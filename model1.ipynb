{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":""},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30299,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e2b74b64","cell_type":"markdown","source":"# Week 5 project: GANs\n\nThis notebook solves the [Iâ€™m Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started)\nKaggle challenge. We are given normal photos along with Monet paintings, and\ninstructed to transform the photos such that they take the style of the Monet\npaintings.\n\nTo be precise, we technically do not need to use the provided photos - the\nultimate goal is to produce Monet style images. Nevertheless, we will be\ntransferring the Monet style to the photos as that is more interesting.\nCycleGAN will be used, as it was designed with that goal in mind. But first,\nlet's introduce GAN first.\n\nGAN stands for Generative Adversarial Network. It is called Generative\nbecause it has a generator component that generates output (usually images)\nfrom input. It is called Adversarial because there is an adversarial\ncomponent called the discriminator which tries to distinguish the generated\noutput from real images. The discriminator's result is used to train the\ngenerator and vice versa.\n\nCycleGAN is a special class of GAN which uses four models instead of two: in\nour case, there will be a generator that transforms a regular photo to Monet\nstyle, and another that transforms Monet style back to a regular photo, and each\ngenerator has a corresponding discriminator. The model has two generators for\na reason: the goal is to transfer the style only, so ideally an image passed\nthrough both generators (a round trip) should look just like the original image.\nThis way of training is very suitable in our case as it does not require\npaired images.\n\nImagine an analogy of an English to French translator. The first generator\ntranslates the text to French, and the other translates French to English.\nEach discriminator validates the translated text is indeed French/English\nrespectively. If the translator is working properly, a text translated from\nEnglish to French and then back to English should be almost the same as the\noriginal English text.\n\nThe first part of this project, which is the extraction of the data and\ntraining the CycleGAN, is built upon [Kaggle's tutorial](https://www.kaggle.com/amyjang/monet-cyclegan-tutorial)\nwith some additions (the EDA section). My main contribution will be comparing\nit to a modified version I call Styled CycleGAN, which adds extra loss functions.","metadata":{}},{"id":"4cec75fc","cell_type":"markdown","source":"# Introduction and Setup\n\nThis notebook utilizes a CycleGAN architecture to add Monet-style to photos. For this tutorial, we will be using the TFRecord dataset. Import the following packages and configure for GPU.\n\nFor more information, check out [TensorFlow](https://www.tensorflow.org/tutorials/generative/cyclegan) and [Keras](https://keras.io/examples/generative/cyclegan/) CycleGAN documentation pages.","metadata":{}},{"id":"04a61b89","cell_type":"code","source":"from os import path, makedirs, environ\nimport shutil\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nif 'KAGGLE_URL_BASE' in environ:\n    from kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\nimport cv2\n\n\n# Configure GPU\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"Using GPU:\", gpus[0])\n        strategy = tf.distribute.MirroredStrategy()\n    except RuntimeError as e:\n        print(e)\nelse:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nprint(tf.__version__)","metadata":{},"outputs":[],"execution_count":null},{"id":"445e22d4","cell_type":"markdown","source":"# Load in the data\n\nWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords.","metadata":{}},{"id":"88648aae","cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path() if 'KAGGLE_URL_BASE' in environ else \"data\"\nIMG_PATH =  \"../tmp\" if 'KAGGLE_URL_BASE' in environ else \"tmp\"\nZIP_NAME = \"/kaggle/working/images\" if 'KAGGLE_URL_BASE' in environ else \"images\"\nmakedirs(IMG_PATH, exist_ok=True)","metadata":{},"outputs":[],"execution_count":null},{"id":"49858cb0","cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{},"outputs":[],"execution_count":null},{"id":"1f9799ec","cell_type":"markdown","source":"All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord.","metadata":{}},{"id":"1811095e","cell_type":"code","source":"IMAGE_SIZE = [256, 256]\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image","metadata":{},"outputs":[],"execution_count":null},{"id":"69ed1486","cell_type":"markdown","source":"Define the function to extract the image from the files.","metadata":{}},{"id":"9967234f","cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{},"outputs":[],"execution_count":null},{"id":"92ab42ab","cell_type":"markdown","source":"Let's load in our datasets.","metadata":{}},{"id":"1c6bbff8","cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)","metadata":{},"outputs":[],"execution_count":null},{"id":"b762ee65","cell_type":"code","source":"example_monet = next(iter(monet_ds))\nexample_photo = next(iter(photo_ds))","metadata":{},"outputs":[],"execution_count":null},{"id":"d550a8e6","cell_type":"markdown","source":"Let's  visualize a photo example and a Monet example.","metadata":{}},{"id":"6d2a8037","cell_type":"code","source":"plt.subplot(121)\nplt.title('Photo')\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(example_monet[0] * 0.5 + 0.5)","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"id":"7959c638","cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\nBefore building our models, let's analyze the dataset to understand its characteristics and properties.","metadata":{}},{"id":"6ac589a8","cell_type":"code","source":"def analyze_dataset(dataset, name):\n    \"\"\"Comprehensive analysis of the dataset\"\"\"\n    print(\"\\n=== %s Dataset Analysis ===\" % name)\n\n    # Count number of samples\n    num_samples = 0\n    for _ in dataset:\n        num_samples += 1\n    print(\"Number of samples: %d\" % num_samples)\n\n    # Analyze first few images\n    sample_count = 0\n    pixel_stats = []\n    for img in dataset.take(20):  # Analyze first 5 batches\n        img_np = img.numpy()\n        pixel_stats.append(img_np)\n        sample_count += 1\n\n        if sample_count <= 3:  # Detailed analysis for first 3 images\n            print(\"\\nSample %d:\" % sample_count)\n            print(\"\\tShape: %s\" % (img_np.shape,))\n            print(\"\\tData type: %s\" % img_np.dtype)\n            print(\"\\tValue range: [%.3f, %.3f]\" % (img_np.min(), img_np.max()))\n            print(\"\\tMean: %.3f, Std: %.3f\" % (img_np.mean(), img_np.std()))\n\n            # Pixel intensity distribution\n            unique, counts = np.unique((img_np * 127.5 + 127.5).astype(np.uint8), return_counts=True)\n            print(\"\\tUnique pixel values: %d\" % len(unique))\n\n    # Overall statistics\n    if pixel_stats:\n        all_pixels = np.concatenate([img.flatten() for img in pixel_stats])\n        print(\"\\nOverall Statistics (%d images):\" % sample_count)\n        print(\"\\tGlobal min: %.3f\" % all_pixels.min())\n        print(\"\\tGlobal max: %.3f\" % all_pixels.max())\n        print(\"\\tGlobal mean: %.3f\" % all_pixels.mean())\n        print(\"\\tGlobal std: %.3f\" % all_pixels.std())\n\n    return num_samples\n\n# Analyze both datasets\nmonet_count = analyze_dataset(load_dataset(MONET_FILENAMES), \"Monet\")\nphoto_count = analyze_dataset(load_dataset(PHOTO_FILENAMES), \"Photo\")","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"id":"52e0ff1b","cell_type":"code","source":"# Visualize pixel intensity distributions\ndef plot_pixel_distributions(monet_ds, photo_ds, num_samples=100):\n    \"\"\"Plot pixel intensity distributions for both datasets\"\"\"\n\n    # Collect pixel values\n    monet_pixels = []\n    photo_pixels = []\n\n    for monet_batch, photo_batch in zip(monet_ds.take(num_samples), photo_ds.take(num_samples)):\n        monet_pixels.extend(monet_batch.numpy().flatten())\n        photo_pixels.extend(photo_batch.numpy().flatten())\n\n    # Plot distributions\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Monet distribution\n    ax1.hist(monet_pixels, bins=50, alpha=0.7, color='blue', density=True)\n    ax1.set_title('Monet Dataset - Pixel Intensity Distribution')\n    ax1.set_xlabel('Pixel Value (normalized)')\n    ax1.set_ylabel('Density')\n    ax1.grid(True, alpha=0.3)\n    ax1.axvline(np.mean(monet_pixels), color='red', linestyle='--', label='Mean: %.3f' % np.mean(monet_pixels))\n    ax1.legend()\n\n    # Photo distribution\n    ax2.hist(photo_pixels, bins=50, alpha=0.7, color='green', density=True)\n    ax2.set_title('Photo Dataset - Pixel Intensity Distribution')\n    ax2.set_xlabel('Pixel Value (normalized)')\n    ax2.set_ylabel('Density')\n    ax2.grid(True, alpha=0.3)\n    ax2.axvline(np.mean(photo_pixels), color='red', linestyle='--', label='Mean: %.3f' % np.mean(photo_pixels))\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return monet_pixels, photo_pixels\n\nmonet_pixels, photo_pixels = plot_pixel_distributions(monet_ds, photo_ds)","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"id":"f606c156","cell_type":"code","source":"# Analyze image similarities within and between domains\ndef analyze_image_similarities(monet_ds, photo_ds, num_samples=50):\n    \"\"\"Analyze structural similarities between images\"\"\"\n    print(\"=== Image Similarity Analysis ===\")\n\n    # Collect sample images\n    monet_samples = []\n    photo_samples = []\n\n    for monet_batch, photo_batch in zip(monet_ds.take(num_samples), photo_ds.take(num_samples)):\n        monet_img = (monet_batch[0].numpy() * 127.5 + 127.5).astype(np.uint8)\n        photo_img = (photo_batch[0].numpy() * 127.5 + 127.5).astype(np.uint8)\n\n        monet_samples.append(cv2.cvtColor(monet_img, cv2.COLOR_RGB2GRAY))\n        photo_samples.append(cv2.cvtColor(photo_img, cv2.COLOR_RGB2GRAY))\n\n    # Calculate within-domain similarities\n    def calculate_within_similarity(images, domain_name):\n        similarities = []\n        for i in range(len(images)):\n            for j in range(i + 1, len(images)):\n                sim = ssim(images[i], images[j])\n                similarities.append(sim)\n\n        print(domain_name, \"- Within-domain similarity:\")\n        print(\"\\tMean SSIM: %.3f\" % np.mean(similarities))\n        print(\"\\tStd SSIM: %.3f\" % np.std(similarities))\n        print(\"\\tRange: [%.3f, %.3f]\" % (np.min(similarities), np.max(similarities)))\n        return similarities\n\n    # Calculate cross-domain similarities\n    def calculate_cross_similarity(images1, images2, domain1, domain2):\n        similarities = []\n        for img1 in images1:\n            for img2 in images2:\n                sim = ssim(img1, img2)\n                similarities.append(sim)\n\n        print(\"%s-%s - Cross-domain similarity:\" % (domain1, domain2))\n        print(\"\\tMean SSIM: %.3f\" % np.mean(similarities))\n        print(\"\\tStd SSIM: %.3f\" % np.std(similarities))\n        return similarities\n\n    monet_similarities = calculate_within_similarity(monet_samples, \"Monet\")\n    photo_similarities = calculate_within_similarity(photo_samples, \"Photo\")\n    cross_similarities = calculate_cross_similarity(monet_samples, photo_samples, \"Monet\", \"Photo\")\n\n    # Plot similarity distributions\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.hist(monet_similarities, bins=20, alpha=0.7, color='blue')\n    plt.title('Monet-Monet Similarities')\n    plt.xlabel('SSIM')\n    plt.ylabel('Frequency')\n\n    plt.subplot(1, 3, 2)\n    plt.hist(photo_similarities, bins=20, alpha=0.7, color='green')\n    plt.title('Photo-Photo Similarities')\n    plt.xlabel('SSIM')\n    plt.ylabel('Frequency')\n\n    plt.subplot(1, 3, 3)\n    plt.hist(cross_similarities, bins=20, alpha=0.7, color='red')\n    plt.title('Monet-Photo Similarities')\n    plt.xlabel('SSIM')\n    plt.ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\nanalyze_image_similarities(monet_ds, photo_ds)","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"id":"9db77bdd","cell_type":"code","source":"# Enhanced visualization of dataset samples\ndef visualize_dataset_samples(monet_ds, photo_ds, num_samples=5):\n    \"\"\"Visualize samples from both domains with enhanced analysis\"\"\"\n\n    fig, axes = plt.subplots(2, num_samples, figsize=(20, 8))\n\n    # Monet samples\n    for i, img in enumerate(monet_ds.take(num_samples)):\n        img_np = (img[0].numpy() * 0.5 + 0.5)  # Convert back to [0,1]\n        axes[0, i].imshow(img_np)\n        axes[0, i].set_title('Monet Sample %d\\nMean: %.3f' % (i+1, img_np.mean()))\n        axes[0, i].axis('off')\n\n        # Add pixel intensity histogram inset\n        inset = axes[0, i].inset_axes([0.6, 0.02, 0.35, 0.25])\n        inset.hist(img_np.flatten(), bins=50, alpha=0.7)\n        inset.set_xticks([])\n        inset.set_yticks([])\n\n    # Photo samples\n    for i, img in enumerate(photo_ds.take(num_samples)):\n        img_np = (img[0].numpy() * 0.5 + 0.5)  # Convert back to [0,1]\n        axes[1, i].imshow(img_np)\n        axes[1, i].set_title('Photo Sample %d\\nMean: %.3f' % (i+1, img_np.mean()))\n        axes[1, i].axis('off')\n\n        # Add pixel intensity histogram inset\n        inset = axes[1, i].inset_axes([0.6, 0.02, 0.35, 0.25])\n        inset.hist(img_np.flatten(), bins=50, alpha=0.7)\n        inset.set_xticks([])\n        inset.set_yticks([])\n\n    plt.suptitle('Dataset Samples with Pixel Distribution Insets', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\nvisualize_dataset_samples(monet_ds, photo_ds)","metadata":{},"outputs":[],"execution_count":null},{"id":"21c48f40","cell_type":"markdown","source":"# Dataset summary and insights\n\nDataset Sizes:\n- Monet paintings: 300 images\n- Photos: 7038 images\n- Total: 7338 images\n\nImage Specifications:\n- Dimensions: 256x256 pixels\n- Channels: 3 (RGB)\n- Format: JPEG from TFRecords\n- Normalization: Scaled to [-1, 1] range\n\nNotes:\n- The monet dataset has an average Pixel Intensity Distribution of around 0,\nwhile the photo dataset has a lower average intensity.\n- TFRecords are a binary storage format by TensorFlow. The images stored\ninside are in JPEG format. The output is also expected to be in JPEG format.\n- The data is not normalized in storage. The `decode_image` function makes\nsure it is in the [-1, 1] range, suitable for tanh layers.","metadata":{}},{"id":"b59438fd","cell_type":"markdown","source":"# Build the generator\n\nWe'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our `downsample` and `upsample` methods.\n\nThe `downsample`, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n\nWe'll be using LayerNormalization instead of instance normalization.","metadata":{}},{"id":"71b41017","cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ndef downsample(filters, size, apply_normalization=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_normalization:\n        result.add(layers.LayerNormalization(epsilon=1e-5))\n\n    result.add(layers.LeakyReLU())\n\n    return result","metadata":{},"outputs":[],"execution_count":null},{"id":"1393aa5b","cell_type":"markdown","source":"`Upsample` does the opposite of downsample and increases the dimensions of the of the image. `Conv2DTranspose` does basically the opposite of a `Conv2D` layer.","metadata":{}},{"id":"06828c60","cell_type":"code","source":"def upsample(filters, size, dropout=0):\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(layers.LayerNormalization(epsilon=1e-5))\n\n    if dropout > 0:\n        result.add(layers.Dropout(dropout))\n\n    result.add(layers.ReLU())\n\n    return result","metadata":{},"outputs":[],"execution_count":null},{"id":"aaab1c4e","cell_type":"markdown","source":"Let's build our generator!\n\nThe generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion.","metadata":{}},{"id":"53bf9438","cell_type":"code","source":"def Generator(dropout=.5):\n    inputs = layers.Input(shape=[256,256,3])\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 4, apply_normalization=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, dropout=dropout), # (bs, 2, 2, 1024)\n        upsample(512, 4, dropout=dropout), # (bs, 4, 4, 1024)\n        upsample(512, 4, dropout=dropout), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{},"outputs":[],"execution_count":null},{"id":"50dbac0f","cell_type":"markdown","source":"# Build the discriminator\n\nThe discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.","metadata":{}},{"id":"bd98337a","cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = layers.LayerNormalization(epsilon=1e-5)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{},"outputs":[],"execution_count":null},{"id":"9723d60f","cell_type":"code","source":"with strategy.scope():\n    monet_generator = Generator() # transforms photos to Monet-esque paintings\n    photo_generator = Generator() # transforms Monet paintings to be more like photos\n\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos","metadata":{},"outputs":[],"execution_count":null},{"id":"d348374d","cell_type":"markdown","source":"Since our generators are not trained yet, the generated Monet-esque photo does not show what is expected at this point.","metadata":{}},{"id":"e212f092","cell_type":"code","source":"to_monet = monet_generator(example_photo)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Monet-esque Photo\")\nplt.imshow(to_monet[0] * 0.5 + 0.5)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"9163e3d6","cell_type":"markdown","source":"# Build the CycleGAN model\n\nWe will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n\nThe losses are defined in the next section.","metadata":{}},{"id":"2ccce536","cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n\n    def call(self, inputs, training=False):\n        # For inference, return the monet generator output\n        return self.m_gen(inputs, training=training)\n\n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n\n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n\n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n\n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }\n\n# Create a sample input of the appropriate shape to use before training the model.\n# Evaluating the model with this input before training will ensure that the weights are initialised with the correct shape.\nsample_input = tf.random.normal([1, 256, 256, 3])","metadata":{"lines_to_next_cell":2},"outputs":[],"execution_count":null},{"id":"8f5710f8","cell_type":"markdown","source":"# Define loss functions\n\nThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.","metadata":{}},{"id":"b8a9dcf4","cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","metadata":{},"outputs":[],"execution_count":null},{"id":"0bca5005","cell_type":"markdown","source":"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","metadata":{}},{"id":"04d64e4b","cell_type":"code","source":"with strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{},"outputs":[],"execution_count":null},{"id":"f117ce6b","cell_type":"markdown","source":"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","metadata":{}},{"id":"68a6c10a","cell_type":"code","source":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","metadata":{},"outputs":[],"execution_count":null},{"id":"3aab90c8","cell_type":"markdown","source":"The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","metadata":{}},{"id":"bd375144","cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{},"outputs":[],"execution_count":null},{"id":"05e831e2","cell_type":"markdown","source":"# Train the CycleGAN\n\nLet's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model.","metadata":{}},{"id":"5dcec975","cell_type":"code","source":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{},"outputs":[],"execution_count":null},{"id":"fe2fc9cf","cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{},"outputs":[],"execution_count":null},{"id":"919c1ec6","cell_type":"code","source":"# Build the model by calling it with a sample input\n_ = cycle_gan_model(sample_input, training=False)\n\ncycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=10\n)\n\n#cycle_gan_model.m_gen.save(\"models/cycle_gan_generator10.keras\")","metadata":{},"outputs":[],"execution_count":null},{"id":"393a4b0a","cell_type":"markdown","source":"# Visualize our Monet-esque photos","metadata":{}},{"id":"bd716d0c","cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"id":"f207c93d","cell_type":"markdown","source":"# CycleGAN vs Styled CycleGAN\n\n## CycleGAN\n\nThe above model was used basically as-is from the tutorial. It used three\ncategories of loss functions:\n1. Adversarial losses: Do the photos/Monet images look real? This encompasses\nthe generator losses of both generators, as well as their respective\ndiscriminator losses.\n2. Can I convert it back? (Remember the translator analogy). This is\naccounted for by the total_cycle_loss.\n3. Identity loss: This helps preserve the colour and tone of the image. When\nthe monet_generator is given a Monet image, the same image is expected at the\noutput. When the photo_generator is given a photo, the same photo is expected\nas well.\n\n## Styled CycleGAN\n\nYou might have sensed a problem with the above model - the output images are\nhardly different from the input photos. If the goal is to make them have\nMonet-esque style, then it is failing horribly. You might say that this is\nbecause it was trained for only 10 epochs, but as we will see below, the\nproblem persists even with 50 epochs.\n\nTherefore, I decided to force the hand of the generator by adding a new loss:\nthe style loss. The style loss is calculated by extracting the texture and\nstyle from the real Monet and generated Monet and comparing them to each\nother.\n\nHow is this achieved? To do this, a pre-trained model called [VGG19](https://arxiv.org/abs/1409.1556)\nis used. The layers of VGG19 can be classified as:\n- Shallow layers: These capture low-level features such as colours, edges,\nbrush strokes, etc.\n- Deep layers: These learn the high-level features - the actual contents of\nthe image.\n\nGiven that, transferring the style can be achieved by adding two losses. The\nfirst is the style loss. A special matrix called the *Gram matrix*\nco-occurrences between the different features in a layer to give us a\nfingerprint of the style. Comparing this fingerprint between the real Monet\nand generated Monet across the shallow layers gives us the style loss.\n\nTo make sure the content is not lost due to the introduction of the style\nloss, an additional content loss is added, which compares the features of the\noriginal photo and generated Monet using the deep layers of the VGG19 model.\nThe combination of both style loss and content loss ensure that only the\nstyle is transferred while the content is preserved.\n\nNote that the use of VGG19 is a form of transfer learning, a concept we\nlearned about in the last lecture of module 3. It is used as a fixed feature\nextractor, and only a part of the layers is used. This is appropriate as the\nnew dataset is relatively small and not very different from the original\ndataset.","metadata":{}},{"id":"7353ebf9","cell_type":"markdown","source":"# Build the Styled CycleGAN model","metadata":{}},{"id":"b2df8afb","cell_type":"code","source":"class StyledCycleGAN(CycleGan):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n        lambda_style=1.0,\n        lambda_content=0.5,\n        **kwargs\n    ):\n        super().__init__(\n            monet_generator, photo_generator,\n            monet_discriminator, photo_discriminator,\n            lambda_cycle, **kwargs\n        )\n        self.lambda_style = lambda_style\n        self.lambda_content = lambda_content\n\n        # Pre-trained VGG for perceptual losses\n        self.vgg = self.build_vgg_feature_extractor()\n\n    def build_vgg_feature_extractor(self):\n        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n        vgg.trainable = False\n\n        # Extract features from specific layers\n        style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n        content_layers = ['block4_conv2']\n\n        outputs = [vgg.get_layer(name).output for name in style_layers + content_layers]\n        return tf.keras.Model(vgg.input, outputs)\n\n    def gram_matrix(self, input_tensor):\n        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n        input_shape = tf.shape(input_tensor)\n        num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n        return result / num_locations\n\n    def style_loss(self, real, generated):\n        real_features = self.vgg(real)\n        generated_features = self.vgg(generated)\n\n        style_loss = 0\n        for real_feat, gen_feat in zip(real_features[:5], generated_features[:5]):\n            real_gram = self.gram_matrix(real_feat)\n            gen_gram = self.gram_matrix(gen_feat)\n            style_loss += tf.reduce_mean(tf.abs(real_gram - gen_gram))\n\n        return style_loss / 5\n\n    def content_loss(self, real, generated):\n        real_features = self.vgg(real)\n        generated_features = self.vgg(generated)\n\n        content_loss = 0\n        for real_feat, gen_feat in zip(real_features[5:], generated_features[5:]):\n            content_loss += tf.reduce_mean(tf.abs(real_feat - gen_feat))\n\n        return content_loss\n\n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n\n        with tf.GradientTape(persistent=True) as tape:\n            # Standard CycleGAN forward pass\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # Additional artistic losses\n            style_loss = self.style_loss(real_monet, fake_monet)\n            content_loss = self.content_loss(real_photo, fake_monet)\n\n            # Standard CycleGAN losses\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + \\\n                             self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # Enhanced generator losses with artistic terms\n            total_monet_gen_loss = (monet_gen_loss + total_cycle_loss +\n                                  self.lambda_style * style_loss +\n                                  self.lambda_content * content_loss)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss\n\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate gradients\n        monet_gen_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n        photo_gen_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n        monet_disc_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n        photo_disc_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n\n        # Apply gradients\n        self.m_gen_optimizer.apply_gradients(zip(monet_gen_gradients, self.m_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_gen_gradients, self.p_gen.trainable_variables))\n        self.m_disc_optimizer.apply_gradients(zip(monet_disc_gradients, self.m_disc.trainable_variables))\n        self.p_disc_optimizer.apply_gradients(zip(photo_disc_gradients, self.p_disc.trainable_variables))\n\n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss,\n            \"style_loss\": style_loss,\n            \"content_loss\": content_loss\n        }","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"id":"31487cc5","cell_type":"markdown","source":"# Train the Styled CycleGAN model","metadata":{}},{"id":"98cc3407","cell_type":"raw","source":"# Create and train Styled CycleGAN\nwith strategy.scope():\n    styled_cyclegan = StyledCycleGAN(\n        Generator(), Generator(),\n        Discriminator(), Discriminator(),\n        lambda_cycle=10,\n        lambda_style=1.0,\n        lambda_content=0.5\n    )\n\n    styled_cyclegan.compile(\n        m_gen_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        p_gen_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        m_disc_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        p_disc_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss\n    )\n\n# Build styled model\n_ = styled_cyclegan(sample_input, training=False)\n\n# Train styled model\nstyled_cyclegan.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=10\n)\n\n#styled_cyclegan.m_gen.save(\"models/style_high_dropout50.keras\")","metadata":{}},{"id":"25ac4169","cell_type":"markdown","source":"# Compare Results from All Models\n\nThe above models were trained with epochs ranging from 10 to 50. The results\nof each CycleGAN and Styled CycleGAN are displayed for comparison, and the\noriginal photos are included for reference.","metadata":{}},{"id":"ec4a45ec","cell_type":"code","source":"def compare_models(test_photos, models_dict):\n    num_models = len(models_dict) + 1\n    num_examples = 4  # Just use a fixed number of examples\n\n    rows = plt.figure(layout=\"constrained\", figsize=(3 * num_examples, 3 * num_models)).subfigures(num_models, 1)\n\n    rows[0].suptitle(\"Original Photos\", fontsize='xx-large')\n    for ax, img in zip(rows[0].subplots(1, num_examples), test_photos.take(num_examples)):\n        # Original photo\n        original = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n        ax.imshow(original)\n        ax.axis('off')\n\n    for row, (model_name, model_path) in zip(rows[1:], models_dict.items()):\n        model = tf.keras.models.load_model(model_path)\n        row.suptitle(model_name, fontsize='xx-large')\n        for ax, img in zip(row.subplots(1, num_examples), test_photos.take(num_examples)):\n            prediction = model(img, training=False)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            ax.imshow(prediction)\n            ax.axis('off')\n\n    # plt.tight_layout()\n    plt.show()\n\n# Compare all models\nmodels_to_compare = {\n    \"CycleGAN, 10 epochs\": \"models/cycle_gan_generator10.keras\",\n    \"CycleGAN, 20 epochs\": \"models/cycle_gan_generator20.keras\",\n    \"CycleGAN, 30 epochs\": \"models/cycle_gan_generator30.keras\",\n    \"CycleGAN, 40 epochs\": \"models/cycle_gan_generator40.keras\",\n    \"CycleGAN, 50 epochs\": \"models/cycle_gan_generator50.keras\",\n    \"Styled CycleGAN, 10 epochs\": \"models/styled_generator10.keras\",\n    \"Styled CycleGAN, 20 epochs\": \"models/styled_generator20.keras\",\n    \"Styled CycleGAN, 30 epochs\": \"models/styled_generator30.keras\",\n    \"Styled CycleGAN, 40 epochs\": \"models/styled_generator40.keras\",\n    \"Styled CycleGAN, 50 epochs\": \"models/styled_generator50.keras\",\n}\n\ncompare_models(photo_ds, models_to_compare)","metadata":{},"outputs":[],"execution_count":null},{"id":"e12a1486","cell_type":"markdown","source":"# Analysis 1\n\nLooking at the plain CycleGAN, we see that some weak patterns appear in the\nimages, but only at epoch 50. The pattern is very weak, and the output can\nhardly be distinguished from the original image.\n\nOn the other hand, the same pattern is very visible in StyledGAN from the\njust 10 epochs of training. However, at 30 epochs, the colour gets distorted\nand the original objects are difficult to recognize. At 40 epochs, the\ncolours are fixed, and the images do look more painting-like but with an\nextra blur. At 50 epochs, the blur is less severe and some of the images do\nlook like a proper Monet painting.\n\n## Problem\n\nStyled CycleGAN is giving better results but there is distortion in the images.\n\n### Hyperparameter tuning\n\nTry training the model, but change the dropout rate from 0.5 to 0.3 or 0.7.","metadata":{}},{"id":"b4c5efdc","cell_type":"raw","source":"# Create and train Styled CycleGAN with a dropout of 0.3 or 0.7\nwith strategy.scope():\n    styled_cyclegan = StyledCycleGAN(\n        Generator(dropout=.7), Generator(dropout=.7),\n        Discriminator(), Discriminator(),\n        lambda_cycle=10,\n        lambda_style=1.0,\n        lambda_content=0.5\n    )\n\n    styled_cyclegan.compile(\n        m_gen_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        p_gen_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        m_disc_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        p_disc_optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss\n    )\n\n# Build the styled model\n_ = styled_cyclegan(sample_input, training=False)\n\n# Train the styled model\nstyled_cyclegan.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=50\n)\n\n#styled_cyclegan.m_gen.save(\"models/style_high_dropout50.keras\")","metadata":{}},{"id":"5228ce19","cell_type":"markdown","source":"# Compare Results from the hyperparameter tuning\n\nThe Styled CycleGAN model was trained with dropouts of 0.3 and 0.7, each with\n40 and 50 epochs.","metadata":{}},{"id":"05c7aa17","cell_type":"code","source":"# Compare hyperparameter tuning results\nmodels_to_compare = {\n    \"Styled CycleGAN, 40 epochs, dropout=.3\": \"models/style_low_dropout40.keras\",\n    \"Styled CycleGAN, 50 epochs, dropout=.3\": \"models/style_low_dropout50.keras\",\n    \"Styled CycleGAN, 40 epochs, dropout=.7\": \"models/style_high_dropout40.keras\",\n    \"Styled CycleGAN, 50 epochs, dropout=.7\": \"models/style_high_dropout50.keras\",\n}\n\ncompare_models(photo_ds, models_to_compare)","metadata":{},"outputs":[],"execution_count":null},{"id":"303068ef","cell_type":"markdown","source":"# Analysis 2\n\nLowering the dropout seems to have been devastating. The images are even less\nrecognizable than the 0.5 dropout version at 30 epochs. On the other hand, at\na dropout of .7, the images do seem clean, but they seem to have lost some of\nthe Monet-esque feel.","metadata":{}},{"id":"4c9686b3","cell_type":"markdown","source":"# Create submission file with best model (Styled CycleGAN)","metadata":{}},{"id":"d6c866ca","cell_type":"raw","source":"i = 1\nfor img in photo_ds:\n    prediction = styled_cyclegan(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = Image.fromarray(prediction)\n    im.save(path.join(IMG_PATH, str(i) + \".jpg\"))\n    i += 1","metadata":{}},{"id":"e83bf853","cell_type":"raw","source":"shutil.make_archive(ZIP_NAME, 'zip', IMG_PATH)","metadata":{}},{"id":"665dacf8","cell_type":"markdown","source":"# Submission results\n\nI submitted different configurations of the model to Kaggle. Below are tables\nof the MiFID scores (lower = better):\n\n| Configuration | CycleGAN | Styled CycleGAN |\n|:-:|:-:|:-:|\n| 50 epochs, dropout=.5 | 70.76 | **59.90** |\n\n| Configuration | 40 epochs | 50 epochs |\n|:-:|:-:|:-:|\n| Styled CycleGAN, dropout=.5 | 92.25 | **59.90** |\n\n| Configuration | dropout=.3 | dropout=.5 | dropout=.7 |\n|:-:|:-:|:-:|:-:|\n| Styled CycleGAN, 50 epochs | 76.75 | **59.90** | 100.65 |","metadata":{}},{"id":"0ef34978","cell_type":"markdown","source":"# Conclusion\n\nFrom the above results, the best configuration is: Styled CycleGAN,\ndropout=0.5, epochs=50.\n\nStyled CycleGAN worked better than normal CycleGAN. This shows that the\naddition of the style_loss and content_loss did indeed help with the style\ntransfer.\n\nOne thing that did not work though is moderating the pattern so it does not\nlook like noise. Adjusting the dropout did not help. In fact, if it showed\none thing, it is that visual clearness does not guarantee a better MiFID score.\n\nA possible reason why the models were struggling with freely imitating the\nMonet style is the need to keep close to the original image. I suspect\ncycle_loss and identity_loss as likely culprits. Had I had a better GPU, I\nwould have likely tried reducing their weights and see if that helps.\n\nAnother approach which could have been successful is replacing CycleGAN\nentirely. The Kaggle competition does not require the Monet-style images to\nbe generated from the provided photos. It would have been interesting to see\nif ignoring the provided photos would have given better results or not.","metadata":{}},{"id":"550f2c83","cell_type":"markdown","source":"# Citations\n\n1. The tutorial recommended by Kaggle\n    - Link: [Amy Jang's notebook](https://www.kaggle.com/amyjang/monet-cyclegan-tutorial)\n\n2. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n    - Authors: Zhu et al. (2017)\n    - Link: [arXiv:1703.10593](https://arxiv.org/abs/1703.10593)\n\n3. Perceptual Losses for Real-Time Style Transfer and Super-Resolution\n    - Authors: Johnson et al. (2016)\n    - Link: [arXiv:1603.08155](https://arxiv.org/abs/1603.08155)\n\n4. A Neural Algorithm of Artistic Style\n    - Authors: Gatys et al. (2015)\n    - Link: [arXiv:1508.06576](https://arxiv.org/abs/1508.06576)\n\n5. Neural Style Transfer from Scratch: A Deep Dive Using VGG19 and Gram Matrices in PyTorch\n    - Link: [https://medium.com/@JohnyOnTheSpot/neural-style-transfer-from-scratch-a-deep-dive-using-vgg19-and-gram-matrices-in-pytorch-598b272e8fc3](https://medium.com/@JohnyOnTheSpot/neural-style-transfer-from-scratch-a-deep-dive-using-vgg19-and-gram-matrices-in-pytorch-598b272e8fc3)","metadata":{}}]}